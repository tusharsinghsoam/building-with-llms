{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c193fd3",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76188aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numexpr\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, FewShotPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.example_selectors import LengthBasedExampleSelector\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2819e121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_text(text):\n",
    "    return display(Markdown(f'<div style=\"font-size: 17px;\">\\n\\n{text}\\n\\n</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fc4012",
   "metadata": {},
   "source": [
    "# LLM Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cc6b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use HuggingFaceEndpoint - automatic routing based on availability\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=300,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "chat_llm = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bed7c1e",
   "metadata": {},
   "source": [
    "# Langchain Quick Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cb5f59",
   "metadata": {},
   "source": [
    "## Asking a simple question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725fa7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build prompt template\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "# we chain together the prompt -> LLM with LCEL (more on this later)\n",
    "llm_chain = prompt | chat_llm\n",
    "\n",
    "question = \"Which NFL team won the Super Bowl in the 2010 season?\"\n",
    "\n",
    "text = (llm_chain.invoke(question))\n",
    "print_text(text.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e354216",
   "metadata": {},
   "source": [
    "## Asking multiple questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0475e82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = [\n",
    "    {'question': \"Which NFL team won the Super Bowl in the 2010 season?\"},\n",
    "    {'question': \"If I am 6 ft 4 inches, how tall am I in centimeters?\"},\n",
    "    {'question': \"Who was the 12th person on the moon?\"},\n",
    "    {'question': \"How many eyes does a blade of grass have?\"}\n",
    "]\n",
    "\n",
    "res = llm_chain.batch(qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096f6bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for question, response in zip(qs, res):\n",
    "    print_text(\"=\"*100)\n",
    "    print_text(f\"QUESTION: {question['question']}\")\n",
    "    print_text(f\"RESPONSE: {response.content}\")\n",
    "    print_text(\"=\"*100 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5e4125",
   "metadata": {},
   "source": [
    "# Langchain Prompt Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99a3955",
   "metadata": {},
   "source": [
    "## Simple prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8f683b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Answer the question based on the context below. If the\n",
    "question cannot be answered using the information provided answer\n",
    "with \"I don't know\".\n",
    "\n",
    "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
    "Their superior performance over smaller models has made them incredibly\n",
    "useful for developers building NLP enabled applications. These models\n",
    "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
    "using the `openai` library, and via Cohere using the `cohere` library.\n",
    "\n",
    "Question: Which libraries and model providers offer LLMs?\n",
    "\n",
    "Answer: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de737795",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = chat_llm.invoke(prompt)\n",
    "print_text(text.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03da8dce",
   "metadata": {},
   "source": [
    "## Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c54c5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based on the context below. If the\n",
    "question cannot be answered using the information provided answer\n",
    "with \"I don't know\".\n",
    "\n",
    "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
    "Their superior performance over smaller models has made them incredibly\n",
    "useful for developers building NLP enabled applications. These models\n",
    "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
    "using the `openai` library, and via Cohere using the `cohere` library.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4408cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt = prompt_template.format(\n",
    "        query=\"Which libraries and model providers offer LLMs?\"\n",
    "    )\n",
    "print_text(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aa7b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StrOutputParser()\n",
    "chain = prompt_template | chat_llm | output_parser\n",
    "\n",
    "response = chain.invoke({\"query\": \"Which libraries and model providers offer LLMs?\"})\n",
    "print_text(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a75565d",
   "metadata": {},
   "source": [
    "## Few-shot Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea079f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=(\n",
    "        \"Create an FAQ in Markdown based on the following questions and answers:\\n\"\n",
    "        \"Q1: What is your return policy?\\n\"\n",
    "        \"A1: We accept returns within 30 days with receipt.\\n\"\n",
    "        \"Q2: Do you ship internationally?\\n\"\n",
    "        \"A2: Yes, we ship to over 50 countries.\\n\"\n",
    "        \"Q3: How can I track my order?\\n\"\n",
    "        \"A3: Use the tracking link in your confirmation email.\"\n",
    "    ),\n",
    "    input_variables=[]  # No variables needed yet\n",
    ")\n",
    "\n",
    "chain = prompt | chat_llm | output_parser\n",
    "response = chain.invoke({})  # Empty dict since no input variables\n",
    "print_text(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0806243d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str = \"\"\"\n",
    "Create a structured Markdown FAQ with anchor links, headers, and formatting conventions for readability.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Example input:\n",
    "Q1: What is your return policy?\n",
    "A1: We accept returns within 30 days with the original receipt.\n",
    "Q2: Do you ship internationally?\n",
    "A2: Yes, we ship to over 50 countries worldwide.\n",
    "Q3: How can I track my order?\n",
    "A3: After your order is shipped, you'll receive a tracking link via email.\n",
    "\n",
    "Example Output:\n",
    "\n",
    "# Frequently Asked Questions\n",
    "\n",
    "## [1. What is your return policy?](#1-what-is-your-return-policy)\n",
    "\n",
    "We accept returns within **30 days** with the original **receipt**.\n",
    "\n",
    "## [2. Do you ship internationally?](#2-do-you-ship-internationally)\n",
    "\n",
    "Yes, we ship to over **50 countries** worldwide.\n",
    "\n",
    "## [3. How can I track my order?](#3-how-can-i-track-my-order)\n",
    "\n",
    "After your order is shipped, you'll receive a **tracking link** via email.\n",
    "\n",
    "---\n",
    "\n",
    "Now generate the FAQ section for this data:\n",
    "Q1: What payment methods do you accept?\n",
    "A1: We accept Visa, Mastercard, PayPal, and Apple Pay.\n",
    "Q2: Can I change my shipping address after ordering?\n",
    "A2: Only if your order hasn't shipped yet. Contact support ASAP.\n",
    "Q3: Do you offer gift wrapping?\n",
    "A3: Yes! You can select gift wrapping during checkout.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Create a PromptTemplate with no input variables (static prompt)\n",
    "prompt = PromptTemplate(template=prompt_str, input_variables=[])\n",
    "\n",
    "chain = prompt | chat_llm | output_parser\n",
    "response = chain.invoke({})  # Empty dict since no input variables\n",
    "print_text(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63867dcf",
   "metadata": {},
   "source": [
    "## Few Shot Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd59f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified examples with just 2 variables instead of 9\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": (\n",
    "            \"Q1: What is your return policy?\\n\"\n",
    "            \"A1: We accept returns within 30 days with the original receipt.\"\n",
    "        ),\n",
    "        \"output\": (\n",
    "            \"## [1. What is your return policy?](#1-what-is-your-return-policy)\\n\\n\"\n",
    "            \"We accept returns within **30 days** with the original **receipt**.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"input\": (\n",
    "            \"Q2: Do you ship internationally?\\n\"\n",
    "            \"A2: Yes, we ship to over 50 countries worldwide.\"\n",
    "        ),\n",
    "        \"output\": (\n",
    "            \"## [2. Do you ship internationally?](#2-do-you-ship-internationally)\\n\\n\"\n",
    "            \"Yes, we ship to over **50 countries** worldwide.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"input\": (\n",
    "            \"Q3: How can I track my order?\\n\"\n",
    "            \"A3: After your order is shipped, you'll receive a tracking link via email.\"\n",
    "        ),\n",
    "        \"output\": (\n",
    "            \"## [3. How can I track my order?](#3-how-can-i-track-my-order)\\n\\n\"\n",
    "            \"After your order is shipped, you'll receive a **tracking link** via email.\"\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "# Much simpler example template with only 2 variables\n",
    "example_template = \"\"\"Example input:\n",
    "{input}\n",
    "\n",
    "Example Output:\n",
    "# Frequently Asked Questions\\n\\n\n",
    "{output}\n",
    "\"\"\"\n",
    "\n",
    "# Create prompt template for examples\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# Instructions prefix\n",
    "prefix = \"\"\"Create a structured Markdown FAQ with anchor links, headers, and formatting conventions for readability.\n",
    "Make sure to bold key terms and important information in the answers.\n",
    "\n",
    "**Example:**\n",
    "\"\"\"\n",
    "\n",
    "# User input format and instructions\n",
    "suffix = \"\"\"\n",
    "Now generate the FAQ section for this data:\n",
    "{input}\n",
    "\"\"\"\n",
    "\n",
    "# Create the few-shot prompt template\n",
    "faq_few_shot = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b686d2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what the prompt looks like before sending to LLM\n",
    "user_input = \"\"\"Q1: What payment methods do you accept?\n",
    "A1: We accept Visa, Mastercard, PayPal, and Apple Pay.\n",
    "Q2: Can I change my shipping address after ordering?\n",
    "A2: Only if your order hasn't shipped yet. Contact support ASAP.\n",
    "Q3: Do you offer gift wrapping?\n",
    "A3: Yes! You can select gift wrapping during checkout.\"\"\"\n",
    "\n",
    "formatted_prompt = faq_few_shot.format(input=user_input)\n",
    "print_text(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f46b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = faq_few_shot | chat_llm | output_parser\n",
    "\n",
    "# Execute with LCEL chain\n",
    "response = chain.invoke({\"input\": user_input})\n",
    "\n",
    "# Display the formatted FAQ (response is already a clean string)\n",
    "print_text(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a851f5d2",
   "metadata": {},
   "source": [
    "## Dynamic inclusion/exclusion of examples in FewShotPromptTemplate using LengthBasedExampleSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f24dcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note we're adding more FAQ examples to each individual example here\n",
    "# (how much you add isn't necessarily important - just that you're aligning\n",
    "# examples as closely as possible to your use-case)\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"\"\"Q1: What is your return policy?\n",
    "A1: We accept returns within 30 days with the original receipt.\n",
    "Q2: Do you ship internationally?\n",
    "A2: Yes, we ship to over 50 countries worldwide.\n",
    "Q3: How can I track my order?\n",
    "A3: After your order is shipped, you'll receive a tracking link via email.\"\"\",\n",
    "        \n",
    "        \"output\": \"\"\"## [1. What is your return policy?](#1-what-is-your-return-policy)\n",
    "\n",
    "We accept returns within **30 days** with the original **receipt**.\n",
    "\n",
    "## [2. Do you ship internationally?](#2-do-you-ship-internationally)\n",
    "\n",
    "Yes, we ship to over **50 countries** worldwide.\n",
    "\n",
    "## [3. How can I track my order?](#3-how-can-i-track-my-order)\n",
    "\n",
    "After your order is shipped, you'll receive a **tracking link** via email.\n",
    "\n",
    "---\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"\"\"Q1: What payment methods do you accept?\n",
    "A1: We accept Visa, Mastercard, PayPal, and Apple Pay.\n",
    "Q2: Can I change my shipping address after ordering?\n",
    "A2: Only if your order hasn't shipped yet. Contact support ASAP.\n",
    "Q3: Do you offer gift wrapping?\n",
    "A3: Yes! You can select gift wrapping during checkout.\"\"\",\n",
    "        \n",
    "        \"output\": \"\"\"## [1. What payment methods do you accept?](#1-what-payment-methods-do-you-accept)\n",
    "\n",
    "We accept **Visa**, **Mastercard**, **PayPal**, and **Apple Pay**.\n",
    "\n",
    "## [2. Can I change my shipping address after ordering?](#2-can-i-change-my-shipping-address-after-ordering)\n",
    "\n",
    "Only if your order hasn't shipped yet. Contact support **ASAP**.\n",
    "\n",
    "## [3. Do you offer gift wrapping?](#3-do-you-offer-gift-wrapping)\n",
    "\n",
    "Yes! You can select **gift wrapping** during checkout.\n",
    "\n",
    "---\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"\"\"Q1: What are your store hours?\n",
    "A1: We're open Monday through Friday from 9am to 9pm, and weekends from 10am to 7pm.\n",
    "Q2: Do you offer price matching?\n",
    "A2: Yes, we'll match any price from authorized retailers for identical products.\n",
    "Q3: What is your warranty policy?\n",
    "A3: All electronics come with a standard 1-year manufacturer warranty.\"\"\",\n",
    "        \n",
    "        \"output\": \"\"\"## [1. What are your store hours?](#1-what-are-your-store-hours)\n",
    "\n",
    "We're open **Monday through Friday** from **9am to 9pm**, and **weekends** from **10am to 7pm**.\n",
    "\n",
    "## [2. Do you offer price matching?](#2-do-you-offer-price-matching)\n",
    "\n",
    "Yes, we'll match any price from **authorized retailers** for **identical products**.\n",
    "\n",
    "## [3. What is your warranty policy?](#3-what-is-your-warranty-policy)\n",
    "\n",
    "All electronics come with a standard **1-year manufacturer warranty**.\n",
    "\n",
    "---\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"\"\"Q1: How do I contact customer service?\n",
    "A1: You can reach us at support@example.com or call 555-123-4567.\n",
    "Q2: Do you offer same-day delivery?\n",
    "A2: Yes, for orders placed before 2pm in selected metro areas.\n",
    "Q3: How do I cancel an order?\n",
    "A3: Log into your account and cancel within 1 hour of placing the order.\"\"\",\n",
    "        \n",
    "        \"output\": \"\"\"## [1. How do I contact customer service?](#1-how-do-i-contact-customer-service)\n",
    "\n",
    "You can reach us at **support@example.com** or call **555-123-4567**.\n",
    "\n",
    "## [2. Do you offer same-day delivery?](#2-do-you-offer-same-day-delivery)\n",
    "\n",
    "Yes, for orders placed before **2pm** in selected **metro areas**.\n",
    "\n",
    "## [3. How do I cancel an order?](#3-how-do-i-cancel-an-order)\n",
    "\n",
    "Log into your account and cancel within **1 hour** of placing the order.\n",
    "\n",
    "---\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"\"\"Q1: Do you offer student discounts?\n",
    "A1: Yes, students with valid ID receive 15% off all purchases.\n",
    "Q2: What is your privacy policy?\n",
    "A2: We never share your personal information with third parties without consent.\n",
    "Q3: Are your products environmentally friendly?\n",
    "A3: We use recyclable packaging and offer carbon-neutral shipping options.\"\"\",\n",
    "        \n",
    "        \"output\": \"\"\"## [1. Do you offer student discounts?](#1-do-you-offer-student-discounts)\n",
    "\n",
    "Yes, students with valid ID receive **15% off** all purchases.\n",
    "\n",
    "## [2. What is your privacy policy?](#2-what-is-your-privacy-policy)\n",
    "\n",
    "We **never share** your personal information with third parties without consent.\n",
    "\n",
    "## [3. Are your products environmentally friendly?](#3-are-your-products-environmentally-friendly)\n",
    "\n",
    "We use **recyclable packaging** and offer **carbon-neutral shipping** options.\n",
    "\n",
    "---\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"\"\"Q1: How do I apply for a refund?\n",
    "A1: Submit your request through our customer portal with your order number and reason.\n",
    "Q2: Do you have a loyalty program?\n",
    "A2: Yes! Earn 1 point for every dollar spent and redeem for discounts.\n",
    "Q3: What are the system requirements?\n",
    "A3: Our software requires Windows 10/11 or macOS 10.15+, 8GB RAM, and 2GB storage.\"\"\",\n",
    "        \n",
    "        \"output\": \"\"\"## [1. How do I apply for a refund?](#1-how-do-i-apply-for-a-refund)\n",
    "\n",
    "Submit your request through our **customer portal** with your **order number** and reason.\n",
    "\n",
    "## [2. Do you have a loyalty program?](#2-do-you-have-a-loyalty-program)\n",
    "\n",
    "Yes! Earn **1 point** for every dollar spent and redeem for **discounts**.\n",
    "\n",
    "## [3. What are the system requirements?](#3-what-are-the-system-requirements)\n",
    "\n",
    "Our software requires **Windows 10/11** or **macOS 10.15+**, **8GB RAM**, and **2GB storage**.\n",
    "\n",
    "---\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"\"\"Q1: How long does shipping take?\n",
    "A1: Standard shipping takes 3-5 business days, and express shipping takes 1-2 business days.\n",
    "Q2: Do you have physical stores?\n",
    "A2: Yes, we have 12 locations across North America. Find the nearest one on our website.\n",
    "Q3: How do I reset my password?\n",
    "A3: Click 'Forgot Password' on the login page and follow the email instructions.\"\"\",\n",
    "\n",
    "        \"output\": \"\"\"## [1. How long does shipping take?](#1-how-long-does-shipping-take)\n",
    "\n",
    "Standard shipping takes **3-5 business days**, and express shipping takes **1-2 business days**.\n",
    "\n",
    "## [2. Do you have physical stores?](#2-do-you-have-physical-stores)\n",
    "\n",
    "Yes, we have **12 locations** across North America. Find the nearest one on our **website**.\n",
    "\n",
    "## [3. How do I reset my password?](#3-how-do-i-reset-my-password)\n",
    "\n",
    "Click **'Forgot Password'** on the login page and follow the email instructions.\n",
    "\n",
    "---\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"\"\"Q1: What's your wholesale policy?\n",
    "A1: For orders over $500, contact our wholesale department for special pricing.\n",
    "Q2: How can I become a vendor?\n",
    "A2: Fill out the vendor application form on our Partners page for consideration.\n",
    "Q3: Do you offer installation services?\n",
    "A3: Yes, professional installation is available for an additional fee in most areas.\"\"\",\n",
    "        \n",
    "        \"output\": \"\"\"## [1. What's your wholesale policy?](#1-whats-your-wholesale-policy)\n",
    "\n",
    "For orders over **$500**, contact our **wholesale department** for special pricing.\n",
    "\n",
    "## [2. How can I become a vendor?](#2-how-can-i-become-a-vendor)\n",
    "\n",
    "Fill out the **vendor application form** on our **Partners page** for consideration.\n",
    "\n",
    "## [3. Do you offer installation services?](#3-do-you-offer-installation-services)\n",
    "\n",
    "Yes, **professional installation** is available for an additional fee in **most areas**.\n",
    "\n",
    "---\"\"\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b91c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=700  # this sets the max length that examples should be\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d74b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_text = \"There are a total of 8 words here.\\nPlus 6 here, totaling 14 words.\"\n",
    "\n",
    "words = re.split('[\\n ]', some_text)\n",
    "print(words, len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa173b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then create the dynamic prompt template\n",
    "dynamic_faq_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,  # use example_selector instead of examples\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"input\"],  # simplified to just one variable\n",
    "    example_separator=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166b30bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using simplified input format\n",
    "user_input = \"\"\"Q1: What payment methods do you accept?\n",
    "A1: We accept Visa, Mastercard, PayPal, and Apple Pay.\n",
    "Q2: Can I change my shipping address after ordering?\n",
    "A2: Only if your order hasn't shipped yet. Contact support ASAP.\n",
    "Q3: Do you offer gift wrapping?\n",
    "A3: Yes! You can select gift wrapping during checkout.\"\"\"\n",
    "\n",
    "formatted_prompt = dynamic_faq_prompt.format(input=user_input)\n",
    "print_text(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85283c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_chain = dynamic_faq_prompt | chat_llm | output_parser\n",
    "\n",
    "# Execute with LCEL chain\n",
    "response = dynamic_chain.invoke({\"input\": user_input})\n",
    "print_text(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc22874",
   "metadata": {},
   "source": [
    "## LengthBasedExampleSelector working when user input is much longer and using more of the context window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac16b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using simplified input format with very long questions and answers\n",
    "user_input = \"\"\"Q1: What are all the different payment methods that you accept for online purchases, including credit cards, digital wallets, bank transfers, and any special financing options that might be available for customers?\n",
    "A1: We accept a comprehensive range of payment methods to accommodate all our customers' preferences and needs. For credit cards, we accept Visa, Mastercard, American Express, and Discover. We also support digital wallet payments through PayPal, Apple Pay, Google Pay, Samsung Pay, and Amazon Pay. Additionally, we offer bank transfer options including ACH transfers, wire transfers, and direct debit for customers who prefer traditional banking methods. For larger purchases, we provide financing options through Affirm, Klarna, and our own in-house financing program with flexible payment plans ranging from 6 to 36 months. We also accept cryptocurrency payments including Bitcoin, Ethereum, and several other major cryptocurrencies for tech-savvy customers.\n",
    "Q2: Is it possible for me to modify or completely change my shipping address after I have already placed and confirmed my order, and if so, what are the specific conditions, timeframes, and procedures that I need to follow?\n",
    "A2: Yes, it is possible to modify your shipping address, but this depends entirely on the current status of your order in our fulfillment process. If your order has not yet been processed by our warehouse team and is still in 'pending' or 'confirmed' status, you can easily change the shipping address by logging into your account and accessing the order management section. However, once your order enters the 'processing' phase and our warehouse team begins preparing your items for shipment, address changes become much more complicated and may not be possible. If your order has already shipped, unfortunately we cannot redirect the package to a different address, but you can contact the shipping carrier directly to arrange for package interception or redirection services, though additional fees may apply. For the best chance of successful address modification, we strongly recommend contacting our customer support team immediately at support@company.com or calling our toll-free number.\n",
    "Q3: Do you provide gift wrapping services for the items that I purchase, and if you do, what are the different options available, what are the costs involved, and can I include personalized messages or special requests?\n",
    "A3: Absolutely! We offer comprehensive gift wrapping services to make your purchases extra special for any occasion. We have several gift wrapping options available: our standard gift wrap features elegant wrapping paper in various colors and patterns with matching ribbon and a bow for an additional $4.99 per item. Our premium gift wrap option includes luxury wrapping paper, silk ribbon, and decorative embellishments for $9.99 per item. For special occasions, we offer themed wrapping for holidays, birthdays, weddings, and baby showers at $7.99 per item. You can also add personalized gift messages up to 250 characters at no additional cost, and we'll include them on beautiful greeting cards. For an extra $2.99, you can upload custom messages or even photos to be printed on special cards. All gift-wrapped items are carefully packaged to ensure they arrive in perfect condition, and we offer discrete packaging options if you're sending gifts directly to recipients.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7215a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_prompt = dynamic_faq_prompt.format(input=user_input)\n",
    "print_text(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f91976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute with LCEL chain\n",
    "response = dynamic_chain.invoke(user_input)\n",
    "\n",
    "# Display the formatted FAQ (response is already a clean string)\n",
    "print_text(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6e5f44",
   "metadata": {},
   "source": [
    "# Langchain New Prompt Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ad7389",
   "metadata": {},
   "source": [
    "## New Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad78bdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the message templates\n",
    "system_template = \"You are a helpful assistant.\"\n",
    "human_template = \"{input}\"\n",
    "\n",
    "# Create the prompt template\n",
    "prompt = ChatPromptTemplate([\n",
    "    (\"system\", system_template),\n",
    "    (\"human\", human_template)\n",
    "])\n",
    "\n",
    "# Create the chain\n",
    "chain = prompt | chat_llm | StrOutputParser()\n",
    "\n",
    "# Test the chain\n",
    "result = chain.invoke({\"input\": \"Hi AI, how are you today?\"})\n",
    "print_text(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626e9839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create initial messages\n",
    "prompt = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"human\", \"Hi AI, how are you today?\"),\n",
    "    (\"ai\", \"I'm great thank you. How can I help you?\"),\n",
    "    (\"human\", \"I'd like to understand string theory.\")\n",
    "])\n",
    "\n",
    "# Create the chain using LCEL pipe syntax\n",
    "chain = prompt | chat_llm | StrOutputParser()\n",
    "\n",
    "# Get response using LCEL\n",
    "res = chain.invoke({})\n",
    "print_text(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff84dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the follow-up question, we can extend the existing prompt\n",
    "prompt.extend([\n",
    "    (\"ai\", res),  # Previous AI response\n",
    "    (\"human\", \"Why do physicists believe it can produce a 'unified theory'?\")\n",
    "])\n",
    "\n",
    "# Create the chain using LCEL pipe syntax\n",
    "chain = prompt | chat_llm | StrOutputParser()\n",
    "\n",
    "# Get response\n",
    "result = chain.invoke({})\n",
    "print_text(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed28898",
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = [\n",
    "    \"English\",\n",
    "    \"Esperanto\",\n",
    "    \"Spanish\",\n",
    "    # \"French\",\n",
    "    # \"German\",\n",
    "    # \"Italian\",\n",
    "    # \"Portuguese\",\n",
    "    # \"Dutch\",\n",
    "    # \"Russian\",\n",
    "    # \"Chinese (Simplified)\",\n",
    "    # \"Chinese (Traditional)\",\n",
    "    # \"Japanese\",\n",
    "    # \"Korean\",\n",
    "    # \"Arabic\",\n",
    "    # \"Hindi\",\n",
    "    # \"Turkish\",\n",
    "    # \"Swedish\",\n",
    "    # \"Danish\",\n",
    "    # \"Norwegian\",\n",
    "    # \"Finnish\",\n",
    "    # \"Polish\",\n",
    "    # \"Czech\",\n",
    "    # \"Hungarian\",\n",
    "    # \"Greek\",\n",
    "    # \"Hebrew\",\n",
    "    # \"Vietnamese\",\n",
    "    # \"Thai\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272f6e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.chat import HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "\n",
    "# Create the prompt template\n",
    "human_template = HumanMessagePromptTemplate.from_template(\n",
    "    \"Translate this input <INPUT_START> {input} <INPUT_END>  into {language}. Do not include any other text in your response.\"\n",
    ")\n",
    "chat_prompt = ChatPromptTemplate([human_template])\n",
    "\n",
    "# Format with dynamic input\n",
    "chat_prompt_value = chat_prompt.format_prompt(\n",
    "    input=\"I hope when you come the weather will be clement.\", # Extra points if you get the reference.\n",
    "    language=\"Spanish\"\n",
    ")\n",
    "\n",
    "chat_prompt_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d5b36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt_value.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1fb491",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt_value.to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2f9a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_template = HumanMessagePromptTemplate.from_template(\n",
    "    \"Translate this input '{input}' into {language}. Do not include any other text in your response.\"\n",
    ")\n",
    "system_template = SystemMessagePromptTemplate.from_template(\"You are a helpful assistant.\")\n",
    "\n",
    "# Create the chain using LCEL pipe syntax\n",
    "chain = (\n",
    "    ChatPromptTemplate([system_template, human_template])\n",
    "    | chat_llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Loop through each language\n",
    "for language in languages:\n",
    "    print_text(f\"\\n=== Response in {language} ===\")\n",
    "\n",
    "    # Invoke the chain with our inputs\n",
    "    result = chain.invoke({\n",
    "        \"input\": \"I hope when you come the weather will be clement.\",\n",
    "        \"language\": language\n",
    "    })\n",
    "\n",
    "    print_text(result)\n",
    "    print_text(\"=\" * 50)  # Separator for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85fb840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create few-shot examples for technical content formatting\n",
    "system_template = SystemMessagePromptTemplate.from_template(\n",
    "    \"\"\"You are a technical translator. You must maintain the exact same format and structure in your translations.\n",
    "    Only translate the explanatory text, keeping all technical terms, numbers, and formatting unchanged.\n",
    "\n",
    "    Example input and output pairs:\n",
    "\n",
    "    Input: \"Error 404: Page not found\"\n",
    "    Output: \"Error 404: Página no encontrada\"\n",
    "\n",
    "    Input: \"Status: 200 OK\n",
    "    Response: {{\n",
    "        'data': 'success',\n",
    "        'message': 'Operation completed'\n",
    "    }}\"\n",
    "    Output: \"Status: 200 OK\n",
    "    Response: {{\n",
    "        'data': 'success',\n",
    "        'message': 'Operación completada'\n",
    "    }}\"\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Example of a technical input\n",
    "human_template = HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\"Translate this technical information to {language}:\n",
    "\n",
    "    Status: 500 Internal Server Error\n",
    "    Response: {{\n",
    "        'error': 'Database connection failed',\n",
    "        'code': 'DB_001',\n",
    "        'timestamp': '2024-03-20T10:30:00Z'\n",
    "    }}\n",
    "\n",
    "    Technical Note: This error occurs when the application cannot connect to the database.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Create the chain using LCEL pipe syntax\n",
    "chain = (\n",
    "    ChatPromptTemplate([system_template, human_template])\n",
    "    | chat_llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Loop through each language\n",
    "for language in languages:\n",
    "    print_text(f\"\\n=== Technical Translation in {language} ===\")\n",
    "\n",
    "    # Invoke the chain with our input\n",
    "    result = chain.invoke({\"language\": language})\n",
    "\n",
    "    print_text(result)\n",
    "    print_text(\"=\" * 80)  # Separator for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c071aec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import SystemMessage, HumanMessage\n",
    "\n",
    "# Create the system message with examples\n",
    "system_message = SystemMessage(content=\"\"\"You are a technical translator. You must maintain the exact same format and structure in your translations.\n",
    "Only translate the explanatory text, keeping all technical terms, numbers, and formatting unchanged.\n",
    "\n",
    "Example input and output pairs:\n",
    "\n",
    "Input: \"Error 404: Page not found\"\n",
    "Output: \"Error 404: Página no encontrada\"\n",
    "\n",
    "Input: \"Status: 200 OK\n",
    "Response: {\n",
    "    'data': 'success',\n",
    "    'message': 'Operation completed'\n",
    "}\"\n",
    "Output: \"Status: 200 OK\n",
    "Response: {\n",
    "    'data': 'success',\n",
    "    'message': 'Operación completada'\n",
    "}\"\n",
    "\"\"\")\n",
    "\n",
    "# Loop through each language\n",
    "for language in languages:\n",
    "    print_text(f\"\\n=== Technical Translation in {language} ===\")\n",
    "\n",
    "    # Create the human message using f-string\n",
    "    human_message = HumanMessage(content=f\"\"\"Translate this technical information to {language}:\n",
    "\n",
    "    Status: 500 Internal Server Error\n",
    "    Response: {{\n",
    "        'error': 'Database connection failed',\n",
    "        'code': 'DB_001',\n",
    "        'timestamp': '2024-03-20T10:30:00Z'\n",
    "    }}\n",
    "\n",
    "    Technical Note: This error occurs when the application cannot connect to the database.\n",
    "    \"\"\")\n",
    "\n",
    "    # Create messages list\n",
    "    messages = [system_message, human_message]\n",
    "\n",
    "    # Get response\n",
    "    res = chat_llm.invoke(messages)\n",
    "\n",
    "    print_text(res.content)\n",
    "    print_text(\"=\" * 80)  # Separator for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1a7cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create few-shot examples for technical content formatting\n",
    "system_template = SystemMessagePromptTemplate.from_template(\n",
    "    \"\"\"You are a technical translator. You must maintain the exact same format and structure in your translations.\n",
    "    Only translate the explanatory text, keeping all technical terms, numbers, and formatting unchanged.\n",
    "\n",
    "    Example input and output pairs:\n",
    "\n",
    "    Input: \"Error 404: Page not found\"\n",
    "    Output: \"Error 404: Página no encontrada\"\n",
    "\n",
    "    Input: \"Status: 200 OK\n",
    "    Response: {% raw %}{{\n",
    "        'data': 'success',\n",
    "        'message': 'Operation completed'\n",
    "    }}{% endraw %}\"\n",
    "    Output: \"Status: 200 OK\n",
    "    Response: {% raw %}{{\n",
    "        'data': 'success',\n",
    "        'message': 'Operación completada'\n",
    "    }}{% endraw %}\"\n",
    "    \"\"\",\n",
    "    template_format=\"jinja2\"\n",
    ")\n",
    "\n",
    "# Example of a technical input using Jinja2's control structures and filters\n",
    "human_template = HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\"Translate this technical information to {{ language|upper }}:\n",
    "\n",
    "    Status: 500 Internal Server Error\n",
    "    Response: {% raw %}{{\n",
    "        'error': 'Database connection failed',\n",
    "        'code': 'DB_001',\n",
    "        'timestamp': '2024-03-20T10:30:00Z'\n",
    "    }}{% endraw %}\n",
    "\n",
    "    Technical Note: This error occurs when the application cannot connect to the database.\n",
    "\n",
    "    {% if language == 'spanish' %}\n",
    "    Note: Please use formal Spanish for technical documentation.\n",
    "    {% elif language == 'french' %}\n",
    "    Note: Please use formal French for technical documentation.\n",
    "    {% else %}\n",
    "    Note: Please maintain a formal tone in the translation.\n",
    "    {% endif %}\n",
    "\n",
    "    {% for term in technical_terms %}\n",
    "    Keep the term \"{{ term }}\" unchanged in the translation.\n",
    "    {% endfor %}\n",
    "    \"\"\",\n",
    "    template_format=\"jinja2\"\n",
    ")\n",
    "\n",
    "# Create the chain using LCEL pipe syntax\n",
    "chain = (\n",
    "    ChatPromptTemplate.from_messages([system_template, human_template])\n",
    "    | chat_llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Loop through each language\n",
    "for language in languages:\n",
    "    print_text(f\"\\n=== Technical Translation in {language} ===\")\n",
    "\n",
    "    # Invoke the chain with our inputs\n",
    "    result = chain.invoke({\n",
    "        \"language\": language,\n",
    "        \"technical_terms\": ['DB_001', 'Internal Server Error', 'Database connection']\n",
    "    })\n",
    "\n",
    "    print_text(result)\n",
    "    print_text(\"=\" * 80)  # Separator for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ae5747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the formatted prompt for Spanish\n",
    "print_text(\"\\n=== Formatted Prompt for Spanish ===\")\n",
    "\n",
    "# Format the prompts with our inputs\n",
    "formatted_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_template,\n",
    "    human_template\n",
    "]).format_prompt(\n",
    "    language='spanish',\n",
    "    technical_terms=['DB_001', 'Internal Server Error', 'Database connection']\n",
    ")\n",
    "\n",
    "# Print the formatted messages\n",
    "for message in formatted_prompt.to_messages():\n",
    "    print_text(f\"\\n{message.type.upper()} MESSAGE:\")\n",
    "    print_text(\"-\" * 40)\n",
    "    print_text(message.content)\n",
    "    print_text(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa0be21",
   "metadata": {},
   "source": [
    "# Lanchain Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520f15d1",
   "metadata": {},
   "source": [
    "## Simple Chain example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dd75cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to handle calculations\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"Calculate using numexpr, with support for basic math operations.\"\"\"\n",
    "    try:\n",
    "        result = float(numexpr.evaluate(expression))\n",
    "        return f\"The result is: {result}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error in calculation: {str(e)}\"\n",
    "\n",
    "# Create the prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful math assistant. When given a math problem, respond ONLY with the mathematical expression that would solve it. For example, if asked 'What is 2 raised to the 3rd power?', respond only with '2**3'.\"),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "# Wrap our calculation function with RunnableLambda for explicit LCEL pattern\n",
    "calculate_runnable = RunnableLambda(calculate)\n",
    "\n",
    "# Create the chain using LCEL with explicit RunnableLambda\n",
    "math_chain = (\n",
    "    prompt\n",
    "    | chat_llm  # LLM to generate the expression\n",
    "    | StrOutputParser()  # Convert to string\n",
    "    | calculate_runnable  # Our calculation function wrapped in RunnableLambda\n",
    ")\n",
    "\n",
    "# Use the chain with our example\n",
    "response = math_chain.invoke({\n",
    "    \"question\": \"What is 13 raised to the .3432 power?\"\n",
    "})\n",
    "print_text(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b3e3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple prompt without guidance\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "basic_chain = (\n",
    "    prompt\n",
    "    | chat_llm  # LLM tries to calculate directly\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response = basic_chain.invoke({\n",
    "    \"question\": \"What is 13 raised to the .3432 power?\"\n",
    "})\n",
    "print_text(response)  # The LLM tries to calculate it directly and might get it wrong!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e3ce74",
   "metadata": {},
   "source": [
    "## Building Complex Chains with LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae189099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    # replace multiple new lines and multiple spaces with a single one\n",
    "    text = re.sub(r'(\\r\\n|\\r|\\n){2,}', r'\\n', text)\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db304bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a creative writing assistant.\"),\n",
    "    (\"user\", \"\"\"Please paraphrase this text in the style of {style}: {text}\"\"\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b988b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the chain using LCEL\n",
    "style_chain = (\n",
    "    {\n",
    "        \"text\": lambda x: clean_text(x[\"text\"]),  # Extract and clean the text from input dict\n",
    "        \"style\": lambda x: x[\"style\"]  # Extract style from input dict\n",
    "    }\n",
    "    | prompt  # Format with our template\n",
    "    | chat_llm  # Generate creative paraphrase\n",
    "    | StrOutputParser()  # Convert to string\n",
    ")\n",
    "\n",
    "# Our input text with messy spacing\n",
    "input_text = \"\"\"\n",
    "Chains allow us to combine multiple\n",
    "\n",
    "\n",
    "components together to create a single, coherent application.\n",
    "\n",
    "For example, we can create a chain that takes user input,       format it with a PromptTemplate,\n",
    "\n",
    "and then passes the formatted response to an LLM. We can build more complex chains by combining     multiple chains together, or by\n",
    "\n",
    "\n",
    "combining chains with other components.\n",
    "\"\"\"\n",
    "\n",
    "# Run the chain\n",
    "response = style_chain.invoke({\n",
    "    \"text\": input_text,\n",
    "    \"style\": \"a 90s rapper\"\n",
    "})\n",
    "print_text(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d55040",
   "metadata": {},
   "source": [
    "## Using RunnableParallel and RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d6e62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two different analysis prompts\n",
    "sentiment_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a sentiment analysis expert. Analyze the emotional tone.\"),\n",
    "    (\"user\", \"What's the sentiment of: {text}\")\n",
    "])\n",
    "\n",
    "summary_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a summarization expert.\"),\n",
    "    (\"user\", \"Summarize in one sentence: {text}\")\n",
    "])\n",
    "\n",
    "# Use RunnableParallel to run both analyses simultaneously\n",
    "analysis_chain = RunnableParallel(\n",
    "    {\n",
    "        \"sentiment\": sentiment_prompt | chat_llm | StrOutputParser(),\n",
    "        \"summary\": summary_prompt | chat_llm | StrOutputParser(),\n",
    "        \"original\": RunnablePassthrough()  # Pass through the original input\n",
    "    }\n",
    ")\n",
    "\n",
    "# Test it\n",
    "sample_text = {\"text\": \"The product exceeded my expectations. Great quality!\"}\n",
    "results = analysis_chain.invoke(sample_text)\n",
    "\n",
    "print_text(\"Sentiment:\", results[\"sentiment\"])\n",
    "print_text(\"Summary:\", results[\"summary\"])\n",
    "print_text(\"Original:\", results[\"original\"][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e827b3",
   "metadata": {},
   "source": [
    "## Batch Processing with LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6d9df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple question-answering chain\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer concisely.\"),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "qa_chain = qa_prompt | chat_llm | StrOutputParser()\n",
    "\n",
    "# Batch of questions\n",
    "questions = [\n",
    "    {\"question\": \"What is the capital of France?\"},\n",
    "    {\"question\": \"Who wrote Romeo and Juliet?\"},\n",
    "    {\"question\": \"What is the speed of light?\"}\n",
    "]\n",
    "\n",
    "# Process all questions in batch\n",
    "answers = qa_chain.batch(questions)\n",
    "\n",
    "# Display results\n",
    "for q, a in zip(questions, answers):\n",
    "    print_text(f\"Q: {q['question']}\")\n",
    "    print_text(f\"A: {a}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
