{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87ab2889",
   "metadata": {},
   "source": [
    "# Getting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "267272f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain version: 1.2.6\n",
      "Python 3.11.0\n"
     ]
    }
   ],
   "source": [
    "import langchain as lc\n",
    "print(\"LangChain version:\", lc.__version__)\n",
    "!python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9319df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n",
       "    num_rows: 41584\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"jamescalam/ai-arxiv-chunked\", split=\"train\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "325dc61f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doi</th>\n",
       "      <th>chunk-id</th>\n",
       "      <th>chunk</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>source</th>\n",
       "      <th>authors</th>\n",
       "      <th>categories</th>\n",
       "      <th>comment</th>\n",
       "      <th>journal_ref</th>\n",
       "      <th>primary_category</th>\n",
       "      <th>published</th>\n",
       "      <th>updated</th>\n",
       "      <th>references</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1910.01108</td>\n",
       "      <td>0</td>\n",
       "      <td>DistilBERT, a distilled version of BERT: small...</td>\n",
       "      <td>1910.01108</td>\n",
       "      <td>DistilBERT, a distilled version of BERT: small...</td>\n",
       "      <td>As Transfer Learning from large-scale pre-trai...</td>\n",
       "      <td>http://arxiv.org/pdf/1910.01108</td>\n",
       "      <td>[Victor Sanh, Lysandre Debut, Julien Chaumond,...</td>\n",
       "      <td>[cs.CL]</td>\n",
       "      <td>February 2020 - Revision: fix bug in evaluatio...</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20191002</td>\n",
       "      <td>20200301</td>\n",
       "      <td>[{'id': '1910.01108'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1910.01108</td>\n",
       "      <td>1</td>\n",
       "      <td>loss combining language modeling, distillation...</td>\n",
       "      <td>1910.01108</td>\n",
       "      <td>DistilBERT, a distilled version of BERT: small...</td>\n",
       "      <td>As Transfer Learning from large-scale pre-trai...</td>\n",
       "      <td>http://arxiv.org/pdf/1910.01108</td>\n",
       "      <td>[Victor Sanh, Lysandre Debut, Julien Chaumond,...</td>\n",
       "      <td>[cs.CL]</td>\n",
       "      <td>February 2020 - Revision: fix bug in evaluatio...</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20191002</td>\n",
       "      <td>20200301</td>\n",
       "      <td>[{'id': '1910.01108'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1910.01108</td>\n",
       "      <td>2</td>\n",
       "      <td>in real-time has the potential to enable novel...</td>\n",
       "      <td>1910.01108</td>\n",
       "      <td>DistilBERT, a distilled version of BERT: small...</td>\n",
       "      <td>As Transfer Learning from large-scale pre-trai...</td>\n",
       "      <td>http://arxiv.org/pdf/1910.01108</td>\n",
       "      <td>[Victor Sanh, Lysandre Debut, Julien Chaumond,...</td>\n",
       "      <td>[cs.CL]</td>\n",
       "      <td>February 2020 - Revision: fix bug in evaluatio...</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20191002</td>\n",
       "      <td>20200301</td>\n",
       "      <td>[{'id': '1910.01108'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1910.01108</td>\n",
       "      <td>3</td>\n",
       "      <td>through distillation via the supervision of a ...</td>\n",
       "      <td>1910.01108</td>\n",
       "      <td>DistilBERT, a distilled version of BERT: small...</td>\n",
       "      <td>As Transfer Learning from large-scale pre-trai...</td>\n",
       "      <td>http://arxiv.org/pdf/1910.01108</td>\n",
       "      <td>[Victor Sanh, Lysandre Debut, Julien Chaumond,...</td>\n",
       "      <td>[cs.CL]</td>\n",
       "      <td>February 2020 - Revision: fix bug in evaluatio...</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20191002</td>\n",
       "      <td>20200301</td>\n",
       "      <td>[{'id': '1910.01108'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1910.01108</td>\n",
       "      <td>4</td>\n",
       "      <td>generalization capabilities of the model and h...</td>\n",
       "      <td>1910.01108</td>\n",
       "      <td>DistilBERT, a distilled version of BERT: small...</td>\n",
       "      <td>As Transfer Learning from large-scale pre-trai...</td>\n",
       "      <td>http://arxiv.org/pdf/1910.01108</td>\n",
       "      <td>[Victor Sanh, Lysandre Debut, Julien Chaumond,...</td>\n",
       "      <td>[cs.CL]</td>\n",
       "      <td>February 2020 - Revision: fix bug in evaluatio...</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20191002</td>\n",
       "      <td>20200301</td>\n",
       "      <td>[{'id': '1910.01108'}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          doi chunk-id                                              chunk  \\\n",
       "0  1910.01108        0  DistilBERT, a distilled version of BERT: small...   \n",
       "1  1910.01108        1  loss combining language modeling, distillation...   \n",
       "2  1910.01108        2  in real-time has the potential to enable novel...   \n",
       "3  1910.01108        3  through distillation via the supervision of a ...   \n",
       "4  1910.01108        4  generalization capabilities of the model and h...   \n",
       "\n",
       "           id                                              title  \\\n",
       "0  1910.01108  DistilBERT, a distilled version of BERT: small...   \n",
       "1  1910.01108  DistilBERT, a distilled version of BERT: small...   \n",
       "2  1910.01108  DistilBERT, a distilled version of BERT: small...   \n",
       "3  1910.01108  DistilBERT, a distilled version of BERT: small...   \n",
       "4  1910.01108  DistilBERT, a distilled version of BERT: small...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  As Transfer Learning from large-scale pre-trai...   \n",
       "1  As Transfer Learning from large-scale pre-trai...   \n",
       "2  As Transfer Learning from large-scale pre-trai...   \n",
       "3  As Transfer Learning from large-scale pre-trai...   \n",
       "4  As Transfer Learning from large-scale pre-trai...   \n",
       "\n",
       "                            source  \\\n",
       "0  http://arxiv.org/pdf/1910.01108   \n",
       "1  http://arxiv.org/pdf/1910.01108   \n",
       "2  http://arxiv.org/pdf/1910.01108   \n",
       "3  http://arxiv.org/pdf/1910.01108   \n",
       "4  http://arxiv.org/pdf/1910.01108   \n",
       "\n",
       "                                             authors categories  \\\n",
       "0  [Victor Sanh, Lysandre Debut, Julien Chaumond,...    [cs.CL]   \n",
       "1  [Victor Sanh, Lysandre Debut, Julien Chaumond,...    [cs.CL]   \n",
       "2  [Victor Sanh, Lysandre Debut, Julien Chaumond,...    [cs.CL]   \n",
       "3  [Victor Sanh, Lysandre Debut, Julien Chaumond,...    [cs.CL]   \n",
       "4  [Victor Sanh, Lysandre Debut, Julien Chaumond,...    [cs.CL]   \n",
       "\n",
       "                                             comment journal_ref  \\\n",
       "0  February 2020 - Revision: fix bug in evaluatio...        None   \n",
       "1  February 2020 - Revision: fix bug in evaluatio...        None   \n",
       "2  February 2020 - Revision: fix bug in evaluatio...        None   \n",
       "3  February 2020 - Revision: fix bug in evaluatio...        None   \n",
       "4  February 2020 - Revision: fix bug in evaluatio...        None   \n",
       "\n",
       "  primary_category published   updated              references  \n",
       "0            cs.CL  20191002  20200301  [{'id': '1910.01108'}]  \n",
       "1            cs.CL  20191002  20200301  [{'id': '1910.01108'}]  \n",
       "2            cs.CL  20191002  20200301  [{'id': '1910.01108'}]  \n",
       "3            cs.CL  20191002  20200301  [{'id': '1910.01108'}]  \n",
       "4            cs.CL  20191002  20200301  [{'id': '1910.01108'}]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.to_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "836f5f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.docstore.document import Document\n",
    "\n",
    "docs = []\n",
    "\n",
    "for row in data:\n",
    "    doc = Document(\n",
    "        page_content=row[\"chunk\"],\n",
    "        metadata={\n",
    "            \"title\": row[\"title\"],\n",
    "            \"source\": row[\"source\"],\n",
    "            \"id\": row[\"id\"],\n",
    "            \"chunk-id\": row[\"chunk-id\"],\n",
    "            \"text\": row[\"chunk\"]\n",
    "        }\n",
    "    )\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61234698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'title': 'DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter', 'source': 'http://arxiv.org/pdf/1910.01108', 'id': '1910.01108', 'chunk-id': '0', 'text': 'DistilBERT, a distilled version of BERT: smaller,\\nfaster, cheaper and lighter\\nVictor SANH, Lysandre DEBUT, Julien CHAUMOND, Thomas WOLF\\nHugging Face\\n{victor,lysandre,julien,thomas}@huggingface.co\\nAbstract\\nAs Transfer Learning from large-scale pre-trained models becomes more prevalent\\nin Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains\\nchallenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be ﬁnetuned with good performances on a wide range of tasks like its larger counterparts.\\nWhile most prior work investigated the use of distillation for building task-speciﬁc\\nmodels, we leverage knowledge distillation during the pre-training phase and show\\nthat it is possible to reduce the size of a BERT model by 40%, while retaining 97%\\nof its language understanding capabilities and being 60% faster. To leverage the\\ninductive biases learned by larger models during pre-training, we introduce a triple\\nloss combining language modeling, distillation and cosine-distance losses. Our\\nsmaller, faster and lighter model is cheaper to pre-train and we demonstrate its'}, page_content='DistilBERT, a distilled version of BERT: smaller,\\nfaster, cheaper and lighter\\nVictor SANH, Lysandre DEBUT, Julien CHAUMOND, Thomas WOLF\\nHugging Face\\n{victor,lysandre,julien,thomas}@huggingface.co\\nAbstract\\nAs Transfer Learning from large-scale pre-trained models becomes more prevalent\\nin Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains\\nchallenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be ﬁnetuned with good performances on a wide range of tasks like its larger counterparts.\\nWhile most prior work investigated the use of distillation for building task-speciﬁc\\nmodels, we leverage knowledge distillation during the pre-training phase and show\\nthat it is possible to reduce the size of a BERT model by 40%, while retaining 97%\\nof its language understanding capabilities and being 60% faster. To leverage the\\ninductive biases learned by larger models during pre-training, we introduce a triple\\nloss combining language modeling, distillation and cosine-distance losses. Our\\nsmaller, faster and lighter model is cheaper to pre-train and we demonstrate its')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483f871b",
   "metadata": {},
   "source": [
    "# Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70778006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.1,\n",
    ")\n",
    "chat_llm = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e01a455",
   "metadata": {},
   "source": [
    "# Embedding and Vector DB Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52bd5bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-26 07:32:39.912969: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "prompt = \"Represent this sentence for searching relevant passages: \"\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "query_encode_kwargs = {\"prompt\": prompt}\n",
    "\n",
    "embed = HuggingFaceEmbeddings(\n",
    "    model_name=model_name, \n",
    "    model_kwargs=model_kwargs, \n",
    "    encode_kwargs=encode_kwargs,\n",
    "    query_encode_kwargs=query_encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69f90145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76e889e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import ServerlessSpec\n",
    "\n",
    "spec = ServerlessSpec(\n",
    "    cloud=\"aws\", region=\"us-east-1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9780b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.0,\n",
       " 'metric': 'dotproduct',\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0,\n",
       " 'vector_type': 'dense'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "index_name = \"langchain-multi-query-demo\"\n",
    "existing_indexes = [\n",
    "    index_info[\"name\"] for index_info in pc.list_indexes()\n",
    "]\n",
    "\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in existing_indexes:\n",
    "    # if does not exist, create index\n",
    "    pc.create_index(\n",
    "        index_name,\n",
    "        dimension=384,  # dimensionality of BAAI/bge-small-en\n",
    "        metric='dotproduct',\n",
    "        spec=spec\n",
    "    )\n",
    "    # wait for index to be initialized\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "# connect to index\n",
    "index = pc.Index(index_name)\n",
    "time.sleep(1)\n",
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8085d05a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41584"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03957ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7514766cc234d68be02df1465285ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "batch_size = 100\n",
    "docs = docs[:20000]  # for demo purposes, only use 20000 records\n",
    "\n",
    "for i in tqdm(range(0, len(docs), batch_size)):\n",
    "    i_end = min(len(docs), i+batch_size)\n",
    "    docs_batch = docs[i:i_end]\n",
    "    # get IDs\n",
    "    ids = [f\"{doc.metadata['id']}-{doc.metadata['chunk-id']}\" for doc in docs_batch]\n",
    "    # get text and embed\n",
    "    texts = [d.page_content for d in docs_batch]\n",
    "    embeds = embed.embed_documents(texts=texts)\n",
    "    # get metadata\n",
    "    metadata = [d.metadata for d in docs_batch]\n",
    "    to_upsert = zip(ids, embeds, metadata)\n",
    "    index.upsert(vectors=to_upsert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90391138",
   "metadata": {},
   "source": [
    "# Multi-Query with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c468040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# initialize the vector store object\n",
    "vectorstore = PineconeVectorStore(index=index, embedding=embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "471553a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore.as_retriever(), llm=chat_llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d39a74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging for the queries\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain_classic.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17a4a6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain_classic.retrievers.multi_query:Generated queries: ['Version 1: ', 'What are the key characteristics and features of Llama 2?', 'This version of the question focuses on retrieving documents that describe the essential attributes and capabilities of Llama 2, allowing the user to gain a deeper understanding of its capabilities.', 'Version 2: ', 'Can you provide information about the improvements and advancements made in Llama 2 compared to its predecessor?', 'This version of the question targets documents that highlight the differences and enhancements between Llama 2 and its previous version, enabling the user to identify the new features and capabilities.', 'Version 3: ', 'What are the applications and use cases where Llama 2 can be effectively utilized, and what are its potential benefits in those areas?', 'This version of the question aims to retrieve documents that showcase the practical applications and potential benefits of Llama 2, allowing the user to explore its real-world uses and value proposition.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"tell me about llama 2?\"\n",
    "\n",
    "queried_docs = retriever.invoke(question)\n",
    "len(queried_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66024cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='2010.07079-76', metadata={'chunk-id': '76', 'id': '2010.07079', 'source': 'http://arxiv.org/pdf/2010.07079', 'title': 'Recipes for Safety in Open-domain Chatbots'}, page_content='1.0 0.1% 0.4% 83.4% 0.1% 0.4% 2.3% 0.187\\nBST 2.7B Non-Sequitur (FT) 0.1 1.3% 7.5% 0.2% 0.1% 0.5% 0% 0.186\\n0.3 0.9% 5.6% 12.6% 0.1% 0.7% 0% 0.188\\n0.5 0.9% 3.3% 29.3% 0.1% 0.7% 0.1% 0.187\\n1.0 0.6% 2.1% 49.1% 0.1% 0.7% 0.2% 0.186\\n1.5\\x030.2% 0.9% 66.1% 0.2% 0.9% 0.2% 0.187\\nTable 11: Automatic Safety Metrics for baked-in models , varying the parameter that controls how often safe\\nresponses ﬁre. We report the % of the time those responses are produced for different hyperparameter choices\\n(Safe%). The models marked with\\x03were chosen for human evaluations.\\nModel Non-Seq%\\nTwo-stage models with classiﬁers\\nBST 2.7B + Multi-Turn Safety Cl. 4.9'),\n",
       " Document(id='2211.09110-279', metadata={'chunk-id': '279', 'id': '2211.09110', 'source': 'http://arxiv.org/pdf/2211.09110', 'title': 'Holistic Evaluation of Language Models'}, page_content='notably quite different. We highlight how T0++ (11B) and davinci (175B) in particular demonstrate the\\nopposite trends when comparing their bias win rates to their toxicity win rates. That is, T0++ (11B) is the\\nmost toxic when compared to all other models, but one of the three least (gender) biases, whereas davinci\\n(175B) is one of the four most biased head-to-head, but one of the less toxic models.\\nAccuracy as a function of other variables. To build on these model comparisons, we now consider\\ntrends in model accuracies as a function of other relevant variables. In Figure 27, we report model accuracies\\nas a function of time (i.e. model release date). As a function of time, we see that the release of GPT-3 (Brown\\net al., 2020) clearly establishes a strong baseline for future models across all scenarios, with a distinctive\\nimprovement over T5 (11B). To the extent there is upward movement in accuracy since then, we often see\\nit coming with the introduction of Anthropic-LM v4-s3 (52B) roughly 18 months later in December, 2021\\n(the first model to use reinforcement learning with human feedback of those we evaluate), though we see a\\nsurprisingly clear monotonic improvement over the intermediary period for NaturalQuestions .'),\n",
       " Document(id='2301.00303-28', metadata={'chunk-id': '28', 'id': '2301.00303', 'source': 'http://arxiv.org/pdf/2301.00303', 'title': 'Rethinking with Retrieval: Faithful Large Language Model Inference'}, page_content='Variant II 78.60 54.54\\nTable 5: Comparison of various variations of RR and\\nthe CoT prompting baseline on StrategyQA using evidence paragraphs.\\net al. (2022). For simplicity, we use the pre-trained\\nNLI model released by Nie et al. (2020) to compute the NLI-based metric, rather than ﬁne-tuning\\nT5-11B (Raffel et al., 2020) ourselves. The implementation details of the two variants can be found\\nin Appendix A.4.\\nResults. Table 5 illustrates that the fact selection and fact generation variants of our proposal\\nimprove the faithfulness of the supporting facts in\\nexplanations, leading to increased prediction accuracy compared to the basic approach without\\nvoting. Across all variations of our proposal, we\\nobserve signiﬁcant improvements in both prediction accuracy and the faithfulness of explanations\\nwhen compared to the CoT prompting baseline.\\nThe incorporation of a voting mechanism leads\\nto an increased prediction accuracy of 79:91% for\\nthe basic approach. Comparison with the performance (i.e., 77:73%) of the same approach using retrieved paragraphs rather than evidence paragraphs in Table 1 demonstrates that retrieved paragraphs are also effective for our proposal, as both'),\n",
       " Document(id='2212.08073-26', metadata={'chunk-id': '26', 'id': '2212.08073', 'source': 'http://arxiv.org/pdf/2212.08073', 'title': 'Constitutional AI: Harmlessness from AI Feedback'}, page_content='[Glaese et al., 2022]. We have included these principles in Appendix C]\\n8Note that the harmlessness Elo scores for the RLHF models look much closer to together compared to\\n[Bai et al., 2022]. We suspect this is because for this work we instructed crowdworkers to prefer thoughtfully harmless responses over evasively harmless responses, which likely reduced the scores for HH RLHF and improved the scores\\nfor helpful RLHF.\\n8\\n0 1 2 3 4\\nNumber of Revisions1\\n012PM Score\\nHarmlessness Score\\n0 1 2 3 4\\nNumber of Revisions2\\n1\\n01\\nHelpfulness Score\\n0 1 2 3 4\\nNumber of Revisions1\\n012\\nHH Score\\n101051010\\nParametersFigure 5 Preference Model scores of responses and revisions from helpful RLHF models, evaluated on a\\nset of red team prompts. The scores are evaluated on a 52B preference model trained on (left) harmlessness\\ncomparisons, (center) helpfulness comparisons, and (right) a mixture of all the combined helpful and harmless\\ncomparisons. The preference models used for evaluation here were trained exclusively using human feedback.\\nWe ﬁnd that harmlessness and HH scores improve monotonically with respect to number of revisions, where\\nrevision 0 refers to the initial response, but pure helpfulness scores decrease.\\n0 1 2 3 4'),\n",
       " Document(id='2304.03277-27', metadata={'chunk-id': '27', 'id': '2304.03277', 'source': 'http://arxiv.org/pdf/2304.03277', 'title': 'Instruction Tuning with GPT-4'}, page_content='tasks.\\nThis represents work in progress, and several directions can be explored: (i)Data and model scale .\\nThe GPT-4 data size is 52K and the base LLaMA model size is 7B. Vicuna collects around 700K\\nconversion turns (approximated from the multi-turn ShareGPT data), and uses the 13B LLaMA\\nmodel. Therefore, it would be promising to continue collecting more GPT-4 instruction-following\\ndata, combine with ShareGPT data, and train larger LLaMA models for higher performance. (ii)\\nRLHF . The reward model is only used in the decoding stage, which suggests that comparison data is\\npromising to provide useful feedback for LLM training. It is natural to continue to train LLMs with\\nreward models, for example for reinforcement learning using machine-generated feedback.\\n8\\nACKNOWLEDGMENTS\\nWe thank Guoyin Wang, Haotian Liu and Hao Cheng for valuable discussions and insightful experience sharing on instruction-tuning language models. We thank the LLaMA team for giving us access\\nto their models.\\nREFERENCES\\nAmanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones,\\nNicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory'),\n",
       " Document(id='2302.13971-27', metadata={'chunk-id': '27', 'id': '2302.13971', 'source': 'http://arxiv.org/pdf/2302.13971', 'title': 'LLaMA: Open and Efficient Foundation Language Models'}, page_content='of LLaMA-65B is already able to follow basic instructions, we observe that a very small amount of\\nﬁnetuning improves the performance on MMLU,\\nand further improves the ability of the model to\\nfollow instructions. Since this is not the focus of\\nthis paper, we only conducted a single experiment\\nfollowing the same protocol as Chung et al. (2022)\\nto train an instruct model, LLaMA-I.\\nOPT 30B 26.1\\nGLM 120B 44.8\\nPaLM 62B 55.1\\nPaLM-cont 62B 62.8\\nChinchilla 70B 67.5\\nLLaMA 65B 63.4\\nOPT-IML-Max 30B 43.2\\nFlan-T5-XXL 11B 55.1\\nFlan-PaLM 62B 59.6\\nFlan-PaLM-cont 62B 66.1\\nLLaMA-I 65B 68.9\\nTable 10: Instruction ﬁnetuning – MMLU (5-shot).\\nComparison of models of moderate size with and without instruction ﬁnetuning on MMLU.In Table 10, we report the results of our instruct\\nmodel LLaMA-I on MMLU and compare with existing instruction ﬁnetuned models of moderate'),\n",
       " Document(id='2212.10560-54', metadata={'chunk-id': '54', 'id': '2212.10560', 'source': 'http://arxiv.org/pdf/2212.10560', 'title': 'Self-Instruct: Aligning Language Models with Self-Generated Instructions'}, page_content='and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:\\n//github.com/tatsu-lab/stanford_alpaca .\\nYizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei,\\nAnjana Arunkumar, Arjun Ashok, Arut Selvan\\nDhanasekaran, Atharva Naik, David Stap, Eshaan\\nPathak, Giannis Karamanolakis, Haizhi Gary Lai, IshanPurohit,IshaniMondal,JacobAnderson,Kirby\\nKuznia, Krima Doshi, Maitreya Patel, Kuntal Kumar Pal, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza,\\nPulkitVerma,RavsehajSinghPuri,RushangKaria,\\nShailaja Keyur Sampat, Savan Doshi, Siddhartha\\nMishra, Sujan Reddy, Sumanta Patro, Tanay Dixit,\\nXudong Shen, Chitta Baral, Yejin Choi, Noah A.'),\n",
       " Document(id='2302.13971-17', metadata={'chunk-id': '17', 'id': '2302.13971', 'source': 'http://arxiv.org/pdf/2302.13971', 'title': 'LLaMA: Open and Efficient Foundation Language Models'}, page_content='but BoolQ. Similarly, this model surpasses PaLM540B everywhere but on BoolQ and WinoGrande.\\nLLaMA-13B model also outperforms GPT-3 on\\nmost benchmarks despite being 10 \\x02smaller.\\n3.2 Closed-book Question Answering\\nWe compare LLaMA to existing large language\\nmodels on two closed-book question answering\\nbenchmarks: Natural Questions (Kwiatkowski\\net al., 2019) and TriviaQA (Joshi et al., 2017). For\\nboth benchmarks, we report exact match performance in a closed book setting, i.e., where the models do not have access to documents that contain\\nevidence to answer the question. In Table 4, we\\nreport performance on NaturalQuestions, and in Table 5, we report on TriviaQA. On both benchmarks,\\nLLaMA-65B achieve state-of-the-arts performance\\nin the zero-shot and few-shot settings. More importantly, the LLaMA-13B is also competitive on\\nthese benchmarks with GPT-3 and Chinchilla, despite being 5-10 \\x02smaller. This model runs on a\\nsingle V100 GPU during inference.\\n0-shot 1-shot 5-shot 64-shot\\nGopher 280B 43.5 - 57.0 57.2'),\n",
       " Document(id='2302.13971-0', metadata={'chunk-id': '0', 'id': '2302.13971', 'source': 'http://arxiv.org/pdf/2302.13971', 'title': 'LLaMA: Open and Efficient Foundation Language Models'}, page_content='LLaMA: Open and Efﬁcient Foundation Language Models\\nHugo Touvron\\x03, Thibaut Lavril\\x03, Gautier Izacard\\x03, Xavier Martinet\\nMarie-Anne Lachaux, Timothee Lacroix, Baptiste Rozière, Naman Goyal\\nEric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin\\nEdouard Grave\\x03, Guillaume Lample\\x03\\nMeta AI\\nAbstract\\nWe introduce LLaMA, a collection of foundation language models ranging from 7B to 65B\\nparameters. We train our models on trillions\\nof tokens, and show that it is possible to train\\nstate-of-the-art models using publicly available datasets exclusively, without resorting\\nto proprietary and inaccessible datasets. In\\nparticular, LLaMA-13B outperforms GPT-3\\n(175B) on most benchmarks, and LLaMA65B is competitive with the best models,\\nChinchilla-70B and PaLM-540B. We release\\nall our models to the research community1.\\n1 Introduction\\nLarge Languages Models (LLMs) trained on massive corpora of texts have shown their ability to perform new tasks from textual instructions or from a\\nfew examples (Brown et al., 2020). These few-shot\\nproperties ﬁrst appeared when scaling models to a'),\n",
       " Document(id='2302.13971-14', metadata={'chunk-id': '14', 'id': '2302.13971', 'source': 'http://arxiv.org/pdf/2302.13971', 'title': 'LLaMA: Open and Efficient Foundation Language Models'}, page_content='•Zero-shot. We provide a textual description\\nof the task and a test example. The model\\neither provides an answer using open-ended\\ngeneration, or ranks the proposed answers.\\n•Few-shot. We provide a few examples of the\\ntask (between 1 and 64) and a test example.\\nThe model takes this text as input and generates the answer or ranks different options.\\nWe compare LLaMA with other foundation models, namely the non-publicly available language\\nmodels GPT-3 (Brown et al., 2020), Gopher (Rae\\net al., 2021), Chinchilla (Hoffmann et al., 2022)\\nand PaLM (Chowdhery et al., 2022), as well as\\nthe open-sourced OPT models (Zhang et al., 2022),\\nGPT-J (Wang and Komatsuzaki, 2021), and GPTNeo (Black et al., 2022). In Section 4, we also\\nbrieﬂy compare LLaMA with instruction-tuned\\nmodels such as OPT-IML (Iyer et al., 2022) and\\nFlan-PaLM (Chung et al., 2022).We evaluate LLaMA on free-form generation\\ntasks and multiple choice tasks. In the multiple\\nchoice tasks, the objective is to select the most'),\n",
       " Document(id='2107.03451-57', metadata={'chunk-id': '57', 'id': '2107.03451', 'source': 'http://arxiv.org/pdf/2107.03451', 'title': 'Anticipating Safety Issues in E2E Conversational AI: Framework and Tooling'}, page_content='topics.5InCase 2 , as noted in §4.4, researchers might consider an explicit “dogfooding” step to\\ngather feedback from users.\\n4.6 P OLICIES\\nAn important aspect of release is whether it is possible to design an effective guard-railing policy\\nto both bolster/maintain the positive outcomes while mitigating the effects of any potential negative\\nconsequences.\\nForCase 1 , in which a model is open-sourced to the research community, policies might include\\nrestrictive licensing or release by request only. If released only by request, then researchers who\\nwish to access the model would be required to contact the model owners. This method upholds\\nthe researchers values’ of reproducibility while potentially limiting unintended uses, but incurs a\\npossibly high maintenance cost if many researchers send in requests with detailed plans of use\\nwhich would need to be examined and adjudicated. If multiple model versions exist which might be\\nexpected to have differing impacts, the researchers might consider adopting a staged release policy,\\nas in Solaiman et al. (2019). This would allow further time and information to aid in technical\\ninvestigations prior to releasing the version expected to have highest impact. Such a policy would\\nbe most effective if users had ample opportunity to provide feedback throughout the release stages.'),\n",
       " Document(id='2212.08073-29', metadata={'chunk-id': '29', 'id': '2212.08073', 'source': 'http://arxiv.org/pdf/2212.08073', 'title': 'Constitutional AI: Harmlessness from AI Feedback'}, page_content='012Harmlessness PM Score\\n1 Revisions\\n101051010\\nParameters1\\n012\\n2 Revisions\\n101051010\\nParameters1\\n012\\n3 Revisions\\n101051010\\nParameters1\\n012\\n4 Revisions\\nCritiqued Revision\\nDirect RevisionFigure 7 Comparison of preference model scores (all on the same 52B PM trained on harmlessness) for\\ncritiqued and direct revisions. We ﬁnd that for smaller models, critiqued revisions generally achieve higher\\nharmlessness scores (higher is more harmless), while for larger models they perform similarly, though critiques are always slightly better.\\n3.5 Are Critiques Necessary?\\nWhile our approach requires sampling a critique followed by a revision, we also consider simplifying our\\napproach by skipping the critique step altogether, and instructing the model to generate a revision directly.\\nIn Figure 7, we compare harmlessness PM scores for critiqued- vs direct-revisions. We found that critiqued\\nrevisions achieved better harmlessness scores for small models, but made no noticeable different for large\\nmodels. Furthermore, based on inspecting samples from the 52B, we found that the critiques were sometimes\\nreasonable, but often made inaccurate or overstated criticisms. Nonetheless, the revisions were generally\\nmore harmless than the original response. An example can be seen in Appendix A. For the main results of'),\n",
       " Document(id='2212.08073-28', metadata={'chunk-id': '28', 'id': '2212.08073', 'source': 'http://arxiv.org/pdf/2212.08073', 'title': 'Constitutional AI: Harmlessness from AI Feedback'}, page_content='constitution. In Figure 6, we compare harmlessness PM score for varying number of constitutions. We ﬁnd\\nthat the number of constitutions does not appear to have a signiﬁcant effect on harmlessness score. Nonetheless, we expect that more constitutions leads to more diverse behaviors, although we did not studied this\\nquantitatively in this work. Diversity is particularly valuable to encourage exploration during the subsequent\\nRL training step.\\nNumber of Revisions\\nIn Figure 5 we show preference model scores for both the initial model response and subsequent revisions.\\nWe ﬁnd that the revisions achieve progressively higher harmlessness scores, suggesting that there’s beneﬁt\\nto utilizing further revisions. However, as discussed in our prior work [Bai et al., 2022], preference model\\nscores become less calibrated at higher values, so these results should be taken with a grain of salt.\\nWe also trained a series of SL-CAI models up to various numbers of revisions. In particular, SL-CAI- nis\\ntrained with ﬁnetuned with up to and including the n-th revision, for n= 1;2;3;4.\\n9\\n101051010\\nParameters1\\n012Harmlessness PM Score\\n1 Revisions\\n101051010\\nParameters1\\n012\\n2 Revisions\\n101051010\\nParameters1\\n012\\n3 Revisions'),\n",
       " Document(id='2009.11462-20', metadata={'chunk-id': '20', 'id': '2009.11462', 'source': 'http://arxiv.org/pdf/2009.11462', 'title': 'RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models'}, page_content='GPT-2 language model further. In our generation\\nexperiments, we prepend the <|nontoxic|> token to our prompts.\\n5.2 Decoding-Based Detoxiﬁcation\\nNoting the additional cost of training language\\nmodels further, we explore three detoxifying strategies that only rely on altering the decoding algorithm and are therefore more readily usable by\\nmany practitioners.\\nVocabulary Shifting (V OCAB -SHIFT )Inspired\\nby Eisenstein et al. (2011) and Ghosh et al. (2017),\\nwe learn a 2-dimensional representation of toxicity\\nand non-toxicity for every token in GPT-2 ’s vocabulary, which we then use to boost the likelihood of\\nnon-toxic tokens. Given the language model’s unnormalized probability (logits) over the vocabulary,\\nwe add the term \\x0cW\\x01t, wheret2R2encodes\\n(non-)toxicity, and W2RVrepresents the associations between each token and (non-)toxicity, and\\n\\x0cis the boosting strength. We set \\x0c= 3 for all\\nexperiments. We learn this representation using the\\ntoxicity labels on the balanced corpus described in'),\n",
       " Document(id='2302.13971-22', metadata={'chunk-id': '22', 'id': '2302.13971', 'source': 'http://arxiv.org/pdf/2302.13971', 'title': 'LLaMA: Open and Efficient Foundation Language Models'}, page_content='PaLM and LaMDA (Thoppilan et al., 2022). PaLM\\nand LLaMA were trained on datasets that contain\\na similar number of code tokens.\\nAs show in Table 8, for a similar number\\nof parameters, LLaMA outperforms other general models such as LaMDA and PaLM, which\\nare not trained or ﬁnetuned speciﬁcally for code.\\nLLaMA with 13B parameters and more outperforms LaMDA 137B on both HumanEval and\\nMBPP. LLaMA 65B also outperforms PaLM 62B,\\neven when it is trained longer. The pass@1 results\\nreported in this table were obtained by sampling\\nwith temperature 0.1. The pass@100 and pass@80\\nmetrics were obtained with temperature 0.8. We\\nuse the same method as Chen et al. (2021) to obtain\\nunbiased estimates of the pass@k.\\nIt is possible to improve the performance on code\\nby ﬁnetuning on code-speciﬁc tokens. For instance,\\nPaLM-Coder (Chowdhery et al., 2022) increases\\nthe pass@1 score of PaLM on HumanEval from\\n26.2% for PaLM to 36%. Other models trained'),\n",
       " Document(id='2110.03215-14', metadata={'chunk-id': '14', 'id': '2110.03215', 'source': 'http://arxiv.org/pdf/2110.03215', 'title': 'Towards Continual Knowledge Learning of Language Models'}, page_content='construct U PDATED LAMA which is made up of cloze statements for which answers can be found\\nin both D0andD1, but are conﬂicting.\\nMeasuring Acquisition of New World Knowledge We deﬁne new world knowledge as the information present in D1, but not in D0. To measure new knowledge acquired through continued\\npretraining on D1, we construct N EWLAMA which is made up of detailed cloze statements requiringnewknowledge from D1to correctly answer. We provide two datasets for measuring new world\\nknowledge : NEWLAMA, for which each of the instances is veriﬁed that the answer does not exist\\ninD0, but only in D1, and N EWLAMA-E ASY for which each of the instances does not perfectly\\ncomply with our strict deﬁnition of newworld knowledge due to its creation process, but is used to\\ngenerally measure the new knowledge acquired from continued pretraining on D1at a larger scale.\\n2CC-R ECENT NEWS consists of 221,779 articles ( \\x18168M tokens), which is estimated to be about 750 times\\nsmaller than C4, a cleansed version of the April 2019 Common Crawl dataset (https://commoncrawl.org/) that\\nwas used to initially pretrain the T5 LM (Raffel et al., 2019).'),\n",
       " Document(id='2205.09712-81', metadata={'chunk-id': '81', 'id': '2205.09712', 'source': 'http://arxiv.org/pdf/2205.09712', 'title': 'Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning'}, page_content='For this reason, we report the latter results in the paper.\\n2WikiMultiHop results We evaluated the performance of LLMs on 2WikiMultiHop (Welbl et al.,\\n2018) dev subset using exact string match between the generated and the ground truth answers.\\nIn particular, the generated answer was truncated at the ﬁrst sentence up to \".\", \"?\", \"!\", \";\" or\\nnewline characters following the BigBench generative evaluation protocol (Ghazal et al., 2017). The\\ntwo answers were then both stripped of all punctuation, white space and special characters before\\ncomparison is made. Dataset examples receive a score of 1 if the post-processed answers match\\nexactly, and 0 otherwise.\\nWe found that the models scored 13\\x9362\\x0634\\x933%on average on the original dataset, consisting\\nof questions only. When the context of Wikipedia paragraphs with relevant and irrelevant facts to\\nanswerthequestionwasaddedtothecontext,theperformancedroppedto 1\\x9347\\x0612\\x9302%onaverage.\\nAdding information about the relevant facts within these context paragraphs did not help much,\\nresulting in 1\\x9397\\x0613\\x939%accuracy. On the other hand, adding only the relevant facts extracted from\\nthe underlying knowledge graph triples has more than doubled the models’ performance, resulting in'),\n",
       " Document(id='2112.11446-35', metadata={'chunk-id': '35', 'id': '2112.11446', 'source': 'http://arxiv.org/pdf/2112.11446', 'title': 'Scaling Language Models: Methods, Analysis & Insights from Training Gopher'}, page_content='10810910101011\\nParameters30405060708090100Accuracy (%)\\nClaim-only\\nClaim and gold evidence\\nRandom 3-way\\nSupervised SOTA\\n10810910101011\\nParameters30405060708090100Accuracy (%)\\nClaim-only (REFUTED vs SUPPORTED)\\nClaim-only (REFUTED vs NOTENOUGHINFO)\\nRandom 2-way\\nFigure 3jScaling curves for FEVER. In the claim-only setting (closed-book) there is a persistent\\ntrend in three-way classiﬁcaton accuracy with parameter scale. Breaking down the three classes into\\ntwo pairs, scale beneﬁts mostly the ability to distinguish SUPPORTED vs REFUTED, but not REFUTED\\nversus NOTENOUGHINFO. When gold evidence is provided (open-book) there is a small beneﬁt from\\n7.1Bto280B GopherandperformanceslightlyexceedsthesupervisedSOTA(Kruengkraietal.,2021).\\nFor some of the most well-studied common sense reasoning tasks:Winogrande ,HellaSwag and\\nPIQA,GopherisoutperformedbythelargerMegatron-TuringNLGbyasmallamount(1.2%,0.2%and'),\n",
       " Document(id='2009.01325-93', metadata={'chunk-id': '93', 'id': '2009.01325', 'source': 'http://arxiv.org/pdf/2009.01325', 'title': 'Learning to summarize from human feedback'}, page_content='However, if we use a linear regression (similar to the procedure in Appendix F) to predict what lead-3\\nperformance would be if its average length were reduced to 314 characters, we still ﬁnd a quality\\nof 5.68, modestly higher than the reference summaries. Moreover, for lead-3 to even achieve parity\\nwith the reference summaries seems to call into question the need for abstractive summarization or\\nsophisticated ML methods, since a simple extractive baseline can match a perfect imitation of the\\nreference summaries.\\nWe wanted to understand labeler behavior on these comparisons, to ensure that it was not an error.\\nTo do this, we examined a sample of our labeler’s judgments ourselves. We found that in 20/143\\ncases labelers preferred lead-3 by 3 points or more, and that excluding these datapoints would raise\\nthe relative score of the reference summaries by about 0.5 points.14We were surprised to see the\\nreference summaries performing so poorly in a signiﬁcant fraction of cases, so we looked at labeler’s\\nexplanations and conﬁrmed they made sense.\\nWe found that two features of the reference summaries explained most of its underperformance. First,\\n13 of these 20 summaries omitted one of the key points from the article—the highlights are often'),\n",
       " Document(id='2211.09110-730', metadata={'chunk-id': '730', 'id': '2211.09110', 'source': 'http://arxiv.org/pdf/2211.09110', 'title': 'Holistic Evaluation of Language Models'}, page_content='GPT-3 (OpenAI). GPT-3 (Brown et al., 2020) is a family of autoregressive Transformer language models\\ntrainedon570GBofInternettext(WebText), ofwhichweevaluateada(350M),babbage(1.3B),curie(6.7B),\\nand davinci (175B). The release date and model size for these models are based on Brown et al. (2020).\\nInstructGPT (OpenAI). InstructGPT models are GPT-3 models fine-tuned with human feedback,\\nwhich the authors find to produce models that are better at following English instructions compared to\\nthe original GPT-3 models (Ouyang et al., 2022). These models differ significantly from the models described in the original InstructGPT paper, which is why we generally refer to them by their API names.115\\nWe evaluate text-ada-001, text-babbage-001, text-curie-001, and text-davinci-002. We have contacted the\\nmodel provider, OpenAI, but have not received formal confirmation on the nature of the training procedure\\nfor these models, the appropriate release date to use, or the appropriate model sizes. For this reason we\\ngenerally exclude this information.\\nCodex (OpenAI). Codex models are GPT-3 models fine-tuned on source code from 54 million public'),\n",
       " Document(id='2211.09110-148', metadata={'chunk-id': '148', 'id': '2211.09110', 'source': 'http://arxiv.org/pdf/2211.09110', 'title': 'Holistic Evaluation of Language Models'}, page_content='lu%3Asubject%3Deconometrics%2Cmodel%3Dtogether_t5-11b%2Cdata_augmentation%3Dcanonical%2Cstop%3Dhash%22%2C%20%22mmlu%3Asubject%3Deconometrics%2Cmodel%3Dtogether_ul2%2Cdata_augmentation%3Dcanonical%2Cstop%3Dhash%22%2C%20%22mmlu%3Asubject%3Deconometrics%2Cmodel%3Dtogether_t0pp%2Cdata_augmentation%3Dcan'),\n",
       " Document(id='2304.07327-22', metadata={'chunk-id': '22', 'id': '2304.07327', 'source': 'http://arxiv.org/pdf/2304.07327', 'title': 'OpenAssistant Conversations -- Democratizing Large Language Model Alignment'}, page_content='cases for the technology11. People were in general very happy to have contributed to the project, with\\n94.25% either agreeing or strongly agreeing with the statement \"Overall, I’m glad I have contributed\\nto OpenAssistant.\".2 For about 40%, this has been their very ﬁrst time contributing to a community\\nproject.\\nFigure 4: Demography of 226 respondents\\n6 Experimental Validation\\n6.1 Instruction Tuning\\nTo evaluate and demonstrate the effectiveness of the OpenAssistant Conversations dataset, we focus\\non the development and evaluation of ﬁne-tuned language models based on Pythia [ 2] and LLaMA\\n[1]. Pythia is a state-of-the-art language model with a permissive open-source license, while LLaMA\\nis a powerful language model with a bespoke non-commercial license.\\n8\\nWe release a suite of ﬁne-tuned language models, including instruction-tuned Pythia-12B, LLaMA13B, and LLaMA-30B, which represents our largest model to date. In order to assess the performance\\nof these models, we evaluate the performance of the Pythia-12B model. We have chosen to focus our\\nanalysis on this model due to its open-source nature, which makes it widely accessible and applicable\\nto a diverse range of applications.'),\n",
       " Document(id='2302.13971-29', metadata={'chunk-id': '29', 'id': '2302.13971', 'source': 'http://arxiv.org/pdf/2302.13971', 'title': 'LLaMA: Open and Efficient Foundation Language Models'}, page_content='contains a large proportion of data from the Web,\\nwe believe that it is crucial to determine the potential for our models to generate such content.\\nTo understand the potential harm of LLaMA-65B,\\nwe evaluate on different benchmarks that measure\\ntoxic content production and stereotypes detection.\\nWhile we have selected some of the standard benchmarks that are used by the language model community to indicate some of the issues with these\\nmodels, these evaluations are not sufﬁcient to fully\\nunderstand the risks associated with these models.\\n0 250 500 750 1000 1250 1500203040506070AccuracyTriviaQA\\n0 250 500 750 1000 1250 15005055606570758085HellaSwag\\n0 250 500 750 1000 1250 150005101520253035NaturalQuestions\\n0 250 500 750 1000 1250 1500\\nBillion of tokens40424446485052AccuracySIQA\\n0 250 500 750 1000 1250 1500\\nBillion of tokens50556065707580WinoGrande\\n0 250 500 750 1000 1250 1500\\nBillion of tokens65.067.570.072.575.077.580.082.5PIQA\\nLLaMA 7B\\nLLaMA 13B\\nLLaMA 33B\\nLLaMA 65B')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queried_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ce4957",
   "metadata": {},
   "source": [
    "# Adding the Generation in RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51a836aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  tell me about llama 2?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain_classic.retrievers.multi_query:Generated queries: ['Version 1: ', 'What are the key characteristics and features of Llama 2?', 'This version of the question focuses on retrieving documents that describe the essential attributes and capabilities of Llama 2, rather than just its name. This can help to overcome the limitation of distance-based similarity search, which may not always return relevant results if the query is too specific or contains typos.', 'Version 2: ', 'Can you provide information about the second generation of Llama models?', \"This version of the question adds more context to the original query, specifying that it's about the second generation of Llama models. This can help to disambiguate the query and retrieve documents that are more relevant to the specific topic.\", 'Version 3: ', 'What are the differences and improvements between Llama 2 and its predecessor?', 'This version of the question focuses on the comparison between Llama 2 and its previous version, which can help to retrieve documents that discuss the advancements and changes made in Llama 2. This can provide a more nuanced understanding of the topic and help to overcome the limitation of distance-based similarity search, which may not always return relevant results if the query is too specific or contains typos.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  I don't know.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Create prompt template with source formatting\n",
    "template = \"\"\"You are a helpful assistant who answers user queries using the\n",
    "    contexts provided. If the question cannot be answered using the information\n",
    "    provided say \"I don't know\".\n",
    "\n",
    "    Contexts:\n",
    "    {contexts}\n",
    "\n",
    "    Question: {query}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Create LCEL chain\n",
    "retrieval_chain = (\n",
    "    {\n",
    "        \"query\": lambda x: x[\"query\"],\n",
    "        \"contexts\": lambda x: \"\\n---\\n\".join([d.page_content for d in retriever.invoke(x[\"query\"])])\n",
    "    }\n",
    "    | prompt\n",
    "    | chat_llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"Query: \", question)\n",
    "\n",
    "response = retrieval_chain.invoke({\"query\": question})\n",
    "print(\"Response: \", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "154d7157",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.delete_index(index_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
